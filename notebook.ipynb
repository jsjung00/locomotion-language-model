{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small-LLM (Locomotion Language Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Can textual language models understand / reason about physics and locomotion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Animals have knowledge regarding physics and locomotion\n",
    "\n",
    "![Giraffe walking](media/leg-giraffe.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) In-context prompt learning + Model Predictive Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt GPT with the current state that evolves based on its generation action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 40/40\n",
      "Current state: [  0.139   0.707  -0.353  -0.125  -0.088  -0.096   0.405  -0.209   1.756\n",
      "   1.789   3.842   8.253  -9.059   9.628   0.851 -14.301  -5.83 ]\n",
      "GPT generated action: [-35.625 -35.625   0.      0.      5.     -1.   ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libEGL warning: failed to open /dev/dri/renderD128: Permission denied\n",
      "\n",
      "libEGL warning: failed to open /dev/dri/card0: Permission denied\n",
      "\n",
      "libEGL warning: failed to open /dev/dri/renderD129: Permission denied\n",
      "\n",
      "libEGL warning: failed to open /dev/dri/card1: Permission denied\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<<< Saved video to /home/ubuntu/small-llm/test-decision-transformer/saved_vids/gptwrapper_cheetah_26009.mp4 >>>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gpt_wrapper.main import mpc_with_gpt\n",
    "from visualize import replay_offscreen\n",
    "import os \n",
    "import random \n",
    "\n",
    "# generate some trajectories\n",
    "np_actions = mpc_with_gpt(max_steps=40)\n",
    "\n",
    "# visualize as video\n",
    "replay_offscreen('mujoco/halfcheetah/expert-v0', np_actions, out_path=os.path.join(\"/home/ubuntu/small-llm/test-decision-transformer/saved_vids\", f\"gptwrapper_cheetah_{random.randint(0,100000)}.mp4\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"saved_vids/gptwrapper_cheetah_46742.mp4\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>\n",
    "\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"saved_vids/gptwrapper_cheetah_66609.mp4\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>\n",
    "\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"saved_vids/gptwrapper_cheetah_83689.mp4\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Fine-tuned small LLM (Pythia-410M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We freeze the entire model and only train linear encoder and decoder layers (4M trainable params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<<< Saved video to /home/ubuntu/small-llm/test-decision-transformer/saved_vids/pythia_targetreward_300_cheetah_66979.mp4 >>>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from visualize import viz_driver\n",
    "\n",
    "# Note: we can condition on our target reward \n",
    "viz_driver(\"pythia\", target_rew=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo videos with different reward conditions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-around; align-items: flex-start; flex-wrap: nowrap; overflow-x: auto;\">\n",
    "  <div style=\"text-align: center; min-width: 300px; margin: 0 10px;\">\n",
    "    <h4>Reward Target: 600</h4>\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "      <source src=\"saved_vids/pythia_targetreward_600_cheetah_81075.mp4\" type=\"video/mp4\"> \n",
    "      Your browser does not support the video tag.\n",
    "    </video>\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"text-align: center; min-width: 300px; margin: 0 10px;\">\n",
    "    <h4>Reward Target: 1200</h4>\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "      <source src=\"saved_vids/pythia_targetreward_1200_cheetah_68985.mp4\" type=\"video/mp4\">\n",
    "      Your browser does not support the video tag.\n",
    "    </video>\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"text-align: center; min-width: 300px; margin: 0 10px;\">\n",
    "    <h4>Reward Target: 2400</h4>\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "      <source src=\"saved_vids/pythia_targetreward_2400_cheetah_31850.mp4\" type=\"video/mp4\">\n",
    "      Your browser does not support the video tag.\n",
    "    </video>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Train GPT2 from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following *Decision Transformer (Chen et al. 2021)*, train GPT2 decoder model (700K params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<<< Saved video to /home/ubuntu/small-llm/test-decision-transformer/saved_vids/dt_targetreward_1200_cheetah_13587.mp4 >>>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from visualize import viz_driver\n",
    "\n",
    "# Note: we can condition on our target reward \n",
    "viz_driver(\"dt\", target_rew=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo videos with different reward conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-around; align-items: flex-start; flex-wrap: nowrap; overflow-x: auto;\">\n",
    "  <div style=\"text-align: center; min-width: 320px; margin: 0 10px;\">\n",
    "    <h4>Reward Target: 300</h4>\n",
    "      <video width=\"640\" height=\"480\" controls>\n",
    "      <source src=\"saved_vids/dt_targetreward_300_cheetah_56626.mp4\" type=\"video/mp4\">\n",
    "      Your browser does not support the video tag.\n",
    "    </video>\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"text-align: center; min-width: 320px; margin: 0 10px;\">\n",
    "    <h4>Reward Target: 600</h4>\n",
    "      <video width=\"640\" height=\"480\" controls>\n",
    "      <source src=\"saved_vids/dt_targetreward_600_cheetah_58199.mp4\" type=\"video/mp4\">\n",
    "      Your browser does not support the video tag.\n",
    "    </video>\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"text-align: center; min-width: 320px; margin: 0 10px;\">\n",
    "    <h4>Reward Target: 1200</h4>\n",
    "      <video width=\"640\" height=\"480\" controls>\n",
    "      <source src=\"saved_vids/dt_targetreward_1200_cheetah_44888.mp4\" type=\"video/mp4\">\n",
    "      Your browser does not support the video tag.\n",
    "    </video>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison on fine-tuned frozen LLM with GPT trained from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Model comparison](media/model_comparison.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
